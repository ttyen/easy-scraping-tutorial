{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: scrape Baidu Baike\n",
    "**Here we build a scraper to crawl Baidu Baike from this [page](https://baike.baidu.com/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711) onwards. We store a historical webpage that we have already visited to keep tracking it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import random\n",
    "\n",
    "\n",
    "base_url = \"https://baike.baidu.com\"\n",
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the last sub url in \"his\", print the title and url.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n"
     ]
    }
   ],
   "source": [
    "url = base_url + his[-1]\n",
    "\n",
    "html = urlopen(url).read().decode('utf-8')\n",
    "soup = BeautifulSoup(html, features='lxml')\n",
    "print(soup.find('h1').get_text(), '    url: ', his[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find all sub_urls for baidu baike (item page), randomly select a sub_urls and store it in \"his\". If no valid sub link is found, than pop last url in \"his\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711', '/item/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1']\n"
     ]
    }
   ],
   "source": [
    "# find valid urls\n",
    "sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "\n",
    "if len(sub_urls) != 0:\n",
    "    his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "else:\n",
    "    # no valid sub link found\n",
    "    his.pop()\n",
    "print(his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put everthing together. Random running for 20 iterations. See what we end up with.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 网络爬虫     url:  /item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\n",
      "1 GUI     url:  /item/%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2\n",
      "2 嵌入式     url:  /item/%E5%B5%8C%E5%85%A5%E5%BC%8F\n",
      "3 嵌入式操作系统     url:  /item/%E5%B5%8C%E5%85%A5%E5%BC%8F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F\n",
      "4 嵌入式微处理器     url:  /item/%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%BE%AE%E5%A4%84%E7%90%86%E5%99%A8\n",
      "5 工业控制计算机     url:  /item/%E5%B7%A5%E4%B8%9A%E6%8E%A7%E5%88%B6%E8%AE%A1%E7%AE%97%E6%9C%BA\n",
      "6 板卡     url:  /item/%E6%9D%BF%E5%8D%A1\n",
      "7 驱动程序     url:  /item/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F\n",
      "8 网络协议     url:  /item/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE\n",
      "9 互联网协议     url:  /item/%E4%BA%92%E8%81%94%E7%BD%91%E5%8D%8F%E8%AE%AE\n",
      "10 数据包     url:  /item/%E6%95%B0%E6%8D%AE%E5%8C%85\n",
      "11 局域网     url:  /item/%E5%B1%80%E5%9F%9F%E7%BD%91\n",
      "12 操作系统     url:  /item/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F\n",
      "13 服务器操作系统     url:  /item/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F\n",
      "14 数据库服务器     url:  /item/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9C%8D%E5%8A%A1%E5%99%A8\n",
      "15 计算机     url:  /item/%E8%AE%A1%E7%AE%97%E6%9C%BA\n",
      "16 中央处理器     url:  /item/%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%8D%95%E5%85%83\n",
      "17 整合内存控制器     url:  /item/%E6%95%B4%E5%90%88%E5%86%85%E5%AD%98%E6%8E%A7%E5%88%B6%E5%99%A8\n",
      "18 内存延迟     url:  /item/%E5%86%85%E5%AD%98%E5%BB%B6%E8%BF%9F\n",
      "19 内存频率     url:  /item/%E5%86%85%E5%AD%98%E9%A2%91%E7%8E%87\n"
     ]
    }
   ],
   "source": [
    "his = [\"/item/%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB/5162711\"]\n",
    "\n",
    "for i in range(20):\n",
    "    url = base_url + his[-1]\n",
    "\n",
    "    html = urlopen(url).read().decode('utf-8')\n",
    "    soup = BeautifulSoup(html, features='lxml')\n",
    "    print(i, soup.find('h1').get_text(), '    url: ', his[-1])\n",
    "\n",
    "    # find valid urls\n",
    "    sub_urls = soup.find_all(\"a\", {\"target\": \"_blank\", \"href\": re.compile(\"/item/(%.{2})+$\")})\n",
    "\n",
    "    if len(sub_urls) != 0:\n",
    "        his.append(random.sample(sub_urls, 1)[0]['href'])\n",
    "    else:\n",
    "        # no valid sub link found\n",
    "        his.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
